{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrangling for this project included gathering, assessing, cleaning and analyzing data. The datas were gathered from different source using different data gathering method. the first data were provided as csv file with project, the second data were collected by web scraping using request library from url that was provided in the project and the last data were collected using twitter API and tweepy library. The twitter API was used after acquiring Twitter developer account, and the customer key and secrets as well as token key and secrets were used to gather the data. Unfortunately, the data collected was reccent, and it was saved to json file. The gathered data were then assessed using visually and programmatically. for visual assessment jupyter notebook and Excel were used. during this assessement various issues were idnetified. For programatic assessement  methods such as describe, info, value_counts, sort_values, count_values, nunique, isnull, notnull and more were used. Two tidiness issue and 8 quality issues were idnetified both through visual and programatic assessement, and theses findings were documented on jupyter notebook in markdown cell. the tidiness issues include time and date variables in the same column and similar variable units in different table. quality issues include missing values, erroneous data type, retweets, wrong names, unnecesary prefixes, and extreme outliers. consequently, cleaning were preformed programatically and tidiness issues were fixed first. and prior to clean the original tables were copied and kept safe from unwanted changes. timestamp column was divided into two time and dates column, and two tables were merged together. however, the last table kept independently as it contains tweets recent than Aug 1, 2017, unlike the two. Then followed by clean quality issues. varouis methods were deployed to clean the data with quality issues. this include drop, replace, astype, and strip method was used. and all the quality issues were fixed. The cleaning process included defining the problem, writing the code and testing the outcome. Finally, the cleaned data  were saved in new csv file. and subsequent analysis and visualization was performed. The labraries used for this project includes pandas, numpy, matplotlib, tweepj, json and more. some codes and methods were obtained from online search, and those are are recorded in the project. likewise, code cell was used to preform data gathering, assessing, cleaning, analyizing and visualizing where as markdown cell was used to document the project in addition to inline commenting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
